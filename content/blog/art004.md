---
title: "Various Concepts Around Linear Regression"
date: 2019-12-11T14:43:13+01:00
draft: false
markup: "goldmark"
math: true
---

# Various Concepts Around Linear Regression

# Interpreting Regression Output


#### Slope $(\beta_i)$ / Intercept-$(\beta_0)$

Estimated chage in the average value of Y resulting from 1-unit change in the input feature $X_i.$ Whereas intercept term measured the average Value of Y When X = 0.

#### $R^2$
R-square explains what proprtion of the total variation is explained by the regression equation, which obviously implies that higher R-square values indicates better model.

#### Adjusted $R^2$
Since R square will continue increasing with each additional regressor (x variable) coming in, it tends to overfit the data. Thus, the concept of adj. R-square was derived, which penalizes unnecessary increasing of variables, and does not necessarily increase with the increase in no. of regressors.

#### t-statistic

The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0. We want it be far away from zero as this would indicate we could reject the null hypothesis - that is, we could decleare a relationship between x and y exist.

#### F-Stat

F-Stat is a good indicator of wheather there is a relationship between our predictor and the response variables. The further the F-statistic is from 1 the better it is. However how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors.

#### Residual SE

Residual Standard Error is measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term. The Residual Standard Error is the average amount that the response will deviate from the true regression line.

#### Tolerance

In multiple regression, tolerance is used aas an indicator of multicollinearity. As a point of interest, tolerance may be said to be the opposite of the coefficent of determination. In that sense, tolerance is identical to the coefficient of alienation.

#### VIF

In multiple regression, the variance inflation factor (VIF) is used as an indicator of multicollinearity. Computationally, it is defines as the reciprocal of tolerance: $\frac{1}{1-R^2}$. All other things equal, researchers desire lower levels of VIF.

#### DW Statistic

It is a test statistic used to detect the presence of autocorrelation at lag 1 in the residuals form a regression analysis.

#### Breusch Pagan Test

It is used to test for heteroscedasticity in a linear regression model. It is a chi-squared test, where if the test statistic has a p-value below an appopriate threshold (e.g. p<0.05) then the null hypothesis of homoscedaticity is rejected.

#### Box-Cox-Transformation

Also known as the power-normal distribution. It is the distribution of a random variable X for which the Box-Cox tranformation on X follows a truncated normal distribution. It is a continuous probability distribution having probability density function. Often Box-Cox-Transformation solves the problem of non-linearity in other function of forms.

#  How to to Deal with Overfitting - Linear Regression?

- Regularization refers to the process of making the model less complex which can further allow it to gereralize better (i.e. avoid overfitting) and perform better on new data.

## Ridge Regression 

{{< figure src="/images/RidgeReg.jpg" caption="Geometric Repressentation of Ridge Regression" width="400" height="400" >}}

## Lasso Regression

{{< figure src="/images/LassoReg.jpg" caption="Geometric Repressentation of Lasso Regression" width="400" height="400" >}}

- **Choice of techniques:** So when sould you use Linear Regression, Ridge, Lasso or Elastic Net?
    + It is almost always required to have at least a little bit of regularizatio, so generally one should avoid plain Linear Regression.
    + Ridge is a good default, but in presence of many redundant features, Lasso or Elastic Net should be the first choidce as they tend to reduce the useless fueatures weights down to zero.
    + In general, Elastic Net is preferred over Lasso since Lasso may be a bit unstable when the number of features is greater than the number of trainig instances or in presence of very high multi-collinearity

## Performance Measures - Lineare Regression

#### PRESS Statistic

In statistics, the predicted residual error sum of squares (PRESS) statistic is a form of cross-validation used in regression analysis to provide a summary measure of the fit of model.

#### MSE

Average squared deviation of the predicted outcome from the actual outcome.
$$
MSE = \frac{1}{n} \sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2 
$$

#### RMSE

(RMSE) is the square root of the mean of the squared errors. RMSE it's better for showing bigger deviations,
$$
MSE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2 }
$$

#### MAE

(MAE) is the mean of the absolute value of the errors. It is less sesitive to the occasional very large error because it does not square the errors in the calculation
$$
MAE =  \frac{1}{n} \sum_{i=1}^{n} \|Y_i - \hat{Y}_i\|
$$

#### MAPE

MAPE is the Mean absolute percentage error and it measures the size of the error in percentage terms. MAPE is useful to compare the precision between different volumes uder study.

$$
MAPE = \frac{100}{n} \sum_{i=1}^{n} \bigg| \frac{Y_i - \hat{Y}_i}{Y_i}\bigg|
$$

#### Measure(s) - model comparision:
    
- AIC
    + The magnitude of AIC (Akaike information Criteria) for specific model is less of intrest than the compariosn of AICs for two or more models because AIC is usally used in context of comparing models, not as an absolute criterion in itself.
    + The usual ***rule*** for comparing two or more models is to choose the one with the minimum AIC value.
    
- BIC/SIC
    + Im statistics, the Bayesian Information Criterion (BIC) or Schwarz Criterion (also SBC, SBIC) is a criterion for model selection among finte set of model, the modet with the lowest BIC is prefferd
    + It is based, in part, on the likelihood function and its is closely related to the Akaike Information Criterion (AIC).
